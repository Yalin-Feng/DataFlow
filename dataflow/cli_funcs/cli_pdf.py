#!/usr/bin/env python3
"""
DataFlow PDF2Model CLI Module - dataflow/cli_funcs/cli_pdf.py
PDF to Model training pipeline with init/train/chat commands
"""

import subprocess
import sys
import yaml
import json
import os
import datetime
from pathlib import Path
from colorama import Fore, Style
from dataflow import get_logger
from .paths import DataFlowPath

logger = get_logger()


def run_script_with_args(script_path: Path, description: str, args: list = None, cwd: str = None) -> bool:
    """Run a Python script with arguments and real-time output"""
    print(f"\n{Fore.BLUE}{description}{Style.RESET_ALL}")
    cmd = [sys.executable, str(script_path)]
    if args:
        cmd.extend(args)
    print(f"Running: {' '.join(cmd)}")
    if cwd:
        print(f"Working directory: {cwd}")

    try:
        result = subprocess.run(cmd, cwd=cwd, check=True,
                                stdout=sys.stdout, stderr=sys.stderr, text=True)
        print(f"{Fore.GREEN}âœ… {description} completed{Style.RESET_ALL}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"{Fore.RED}âŒ {description} failed{Style.RESET_ALL}")
        return False


def get_dataflow_script_path(script_name: str) -> Path:
    """Get the path of dataflow built-in scripts"""
    try:
        import dataflow
        dataflow_path = Path(dataflow.__file__).parent

        # PDF2Model è„šæœ¬åœ¨ dataflow/cli_funcs/pdf2model_pipeline/ ç›®å½•ä¸‹
        pdf2model_path = dataflow_path / "cli_funcs" / "pdf2model_pipeline" / script_name
        if pdf2model_path.exists():
            return pdf2model_path

        # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„è·¯å¾„
        possible_dirs = [
            dataflow_path / "templates" / "pdf2model_pipeline",
            dataflow_path / "pipeline_templates"
        ]

        for dir_path in possible_dirs:
            script_path = dir_path / script_name
            if script_path.exists():
                return script_path

        return None
    except:
        return None


def copy_customizable_scripts():
    """Only copy scripts that users might want to customize"""
    print("Step 0: Copying customizable pipeline script...")

    current_dir = Path(os.getcwd())

    try:
        # åªå¤åˆ¶ç”¨æˆ·å¯èƒ½éœ€è¦è‡ªå®šä¹‰çš„è„šæœ¬
        scripts_to_copy = [
            "pdf_to_qa_pipeline.py"  # ç”¨æˆ·å¯èƒ½éœ€è¦ä¿®æ”¹ vLLM/sglang é…ç½®
        ]

        import shutil
        copied_files = []

        for script_name in scripts_to_copy:
            source_path = get_dataflow_script_path(script_name)
            if source_path is None:
                print(f"Warning: Template not found: {script_name}")
                continue

            target_file = current_dir / script_name

            shutil.copy2(source_path, target_file)
            copied_files.append(script_name)
            print(f"Copied: {script_name}")

        if copied_files:
            print(f"Successfully copied {len(copied_files)} customizable script(s)")
            print("You can now modify these files (e.g., switch vLLM/sglang in pdf_to_qa_pipeline.py)")
            return True
        else:
            print("No customizable scripts were copied")
            return False

    except Exception as e:
        print(f"Failed to copy scripts: {e}")
        return False


def create_train_config_yaml(cache_path="./", model_name_or_path="Qwen/Qwen2.5-7B-Instruct"):
    """Create train_config.yaml file using built-in LlamaFactory configuration"""
    cache_path_obj = Path(cache_path)
    if not cache_path_obj.is_absolute():
        caller_cwd = Path(os.environ.get('PWD', os.getcwd()))
        cache_path_obj = caller_cwd / cache_path_obj

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_dir_name = f"pdf2model_cache_{timestamp}"  # æ”¹ä¸ºpdf2model_cacheå‰ç¼€

    cache_dir = cache_path_obj / ".cache"
    cache_dir.mkdir(parents=True, exist_ok=True)
    config_file = cache_dir / "train_config.yaml"

    try:
        # ä½¿ç”¨å†…ç½®çš„ LlamaFactory.py è·å–é»˜è®¤é…ç½®
        llamafactory_script_path = get_dataflow_script_path("llama_factory_trainer.py")
        if llamafactory_script_path is None:
            print("Built-in llama_factory_trainer.py not found")
            return None

        import importlib.util
        spec = importlib.util.spec_from_file_location("llamafactory_trainer", llamafactory_script_path)
        llamafactory_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(llamafactory_module)

        # åˆ›å»ºtrainerå®ä¾‹å¹¶è·å–é»˜è®¤é…ç½®
        trainer = llamafactory_module.LlamaFactoryTrainer(str(config_file), str(cache_path_obj))
        config = trainer.get_default_config()

        # åªæ›´æ–°å¿…è¦çš„åŠ¨æ€å‚æ•°
        config["model_name_or_path"] = model_name_or_path
        config["output_dir"] = str(cache_path_obj / ".cache" / "saves" / model_dir_name)
        config["dataset_dir"] = str(cache_path_obj / ".cache" / "data")

        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®æ¨¡æ¿
        if "qwen" in model_name_or_path.lower():
            config["template"] = "qwen"
        elif "llama" in model_name_or_path.lower():
            config["template"] = "llama3"
        elif "chatglm" in model_name_or_path.lower():
            config["template"] = "chatglm3"
        elif "baichuan" in model_name_or_path.lower():
            config["template"] = "baichuan2"

        # ä¿å­˜é…ç½®
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f,
                      default_flow_style=False,
                      allow_unicode=True,
                      sort_keys=False,
                      indent=2)

        print(f"train_config.yaml created: {config_file}")
        print(f"Model will be saved to: {model_dir_name}")
        return str(config_file)

    except Exception as e:
        print(f"Failed to create train_config.yaml: {e}")
        return None


def verify_environment():
    """Verify runtime environment"""
    print("Checking environment...")

    missing_deps = []

    try:
        import llamafactory
        print("âœ… LlamaFactory installed")
    except ImportError:
        missing_deps.append("llamafactory[torch,metrics]")

    try:
        import yaml
        print("âœ… PyYAML installed")
    except ImportError:
        missing_deps.append("pyyaml")

    if missing_deps:
        print(f"âŒ Missing dependencies: {', '.join(missing_deps)}")
        print(f"Install with: pip install {' '.join(missing_deps)}")
        return False

    return True


def check_required_files():
    """Check if required built-in scripts exist"""
    # æ£€æŸ¥æ‰€æœ‰éœ€è¦çš„å†…ç½®è„šæœ¬
    required_scripts = [
        "path_to_jsonl_script.py",
        "merge_filter_qa_pairs.py",
        "llama_factory_trainer.py"
    ]

    missing_scripts = []
    for script in required_scripts:
        script_path = get_dataflow_script_path(script)
        if script_path is None:
            missing_scripts.append(script)
        else:
            print(f"âœ… Found built-in script: {script}")

    if missing_scripts:
        print(f"âŒ Missing built-in scripts: {', '.join(missing_scripts)}")
        print("These should be part of the dataflow installation")
        return False

    # æ£€æŸ¥ç”¨æˆ·ç›®å½•ä¸‹æ˜¯å¦æœ‰å¯è‡ªå®šä¹‰çš„è„šæœ¬
    current_dir = Path(os.getcwd())
    customizable_script = current_dir / "pdf_to_qa_pipeline.py"
    if customizable_script.exists():
        print("âœ… Found customizable script: pdf_to_qa_pipeline.py")
    else:
        print("âŒ Missing customizable script: pdf_to_qa_pipeline.py")
        print("Run 'dataflow pdf2model init' first")
        return False

    return True


def cli_pdf2model_init(cache_path: str = "./", model_name: str = "Qwen/Qwen2.5-7B-Instruct") -> bool:
    """
    PDF2Model initialization:
    0. Copy only customizable scripts to current directory
    1. Create train_config.yaml in .cache directory
    """
    print("Starting PDF2Model initialization...")
    print(f"Cache directory: {cache_path}")
    print(f"Model: {model_name}")
    print(f"Output directory: pdf2model_cache_<timestamp>")  # æ›´æ–°è¾“å‡ºç›®å½•æ˜¾ç¤º
    print("-" * 60)

    if not verify_environment():
        return False

    try:
        # Step 0: Copy only customizable scripts
        if not copy_customizable_scripts():
            return False

        # Step 1: Create training configuration
        print("Step 1: Creating training configuration...")
        config_file = create_train_config_yaml(cache_path, model_name)

        if config_file:
            print("PDF2Model initialization completed!")
            return True
        else:
            print("Failed to create training configuration")
            return False

    except Exception as e:
        print(f"Initialization failed: {e}")
        return False


def get_latest_model_dir(cache_path_obj):
    """è·å–æœ€æ–°çš„æ¨¡å‹ç›®å½•ï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰"""
    saves_dir = cache_path_obj / ".cache" / "saves"
    if not saves_dir.exists():
        return None

    # æŸ¥æ‰¾æ‰€æœ‰ pdf2model_cache_ å¼€å¤´çš„ç›®å½•
    model_dirs = []
    for dir_path in saves_dir.iterdir():
        if dir_path.is_dir() and dir_path.name.startswith('pdf2model_cache_'):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ­£ç¡®çš„æ—¶é—´æˆ³æ ¼å¼ (YYYYMMDD_HHMMSS)
            timestamp_part = dir_path.name.replace('pdf2model_cache_', '')
            if len(timestamp_part) == 15 and timestamp_part[8] == '_':
                date_part = timestamp_part[:8]
                time_part = timestamp_part[9:]
                if date_part.isdigit() and time_part.isdigit() and len(time_part) == 6:
                    model_dirs.append(dir_path)

    if not model_dirs:
        return None

    # æŒ‰åç§°æ’åºï¼ˆæ—¶é—´æˆ³ä¼šè‡ªç„¶æ’åºï¼‰
    model_dirs.sort(key=lambda x: x.name, reverse=True)
    return model_dirs[0]


def cli_pdf2model_train(lf_yaml: str = ".cache/train_config.yaml", cache_path: str = "./") -> bool:
    """
    Start PDF2Model training using mix of built-in and user scripts
    """
    print("Starting PDF2Model training...")

    current_dir = Path(os.getcwd())

    cache_path_obj = Path(cache_path)
    if not cache_path_obj.is_absolute():
        cache_path_obj = current_dir / cache_path_obj

    config_path_obj = Path(lf_yaml)
    if not config_path_obj.is_absolute():
        config_path_obj = current_dir / config_path_obj

    if not verify_environment():
        return False

    if not check_required_files():
        return False

    if not config_path_obj.exists():
        print(f"Training config file not found: {config_path_obj}")
        print(f"{Style.BRIGHT}Run 'dataflow pdf2model init' first")
        return False

    print("-" * 60)

    try:
        # Step 1: PDF Detection - ä½¿ç”¨å†…ç½®è„šæœ¬
        script1_path = get_dataflow_script_path("path_to_jsonl_script.py")
        args1 = ["./", "--output", str(cache_path_obj / ".cache" / "gpu" / "pdf_list.jsonl")]
        if not run_script_with_args(script1_path, "Step 1: PDF Detection", args1, cwd=str(current_dir)):
            return False

        # Step 2: Data Processing - ä½¿ç”¨ç”¨æˆ·ç›®å½•ä¸‹çš„è„šæœ¬
        script2 = current_dir / "pdf_to_qa_pipeline.py"
        args2 = ["--cache", cache_path]
        if not run_script_with_args(script2, "Step 2: Data Processing", args2, cwd=str(current_dir)):
            return False

        # Step 3: Data Conversion - ä½¿ç”¨å†…ç½®è„šæœ¬
        script3_path = get_dataflow_script_path("merge_filter_qa_pairs.py")
        args3 = ["--cache", cache_path]
        if not run_script_with_args(script3_path, "Step 3: Data Conversion", args3, cwd=str(current_dir)):
            return False

        # Step 4: Training - ä½¿ç”¨å†…ç½®è„šæœ¬
        script4_path = get_dataflow_script_path("llama_factory_trainer.py")
        args4 = ["--config", str(config_path_obj), "--cache", cache_path]
        if not run_script_with_args(script4_path, "Step 4: Training", args4, cwd=str(current_dir)):
            return False

        # æ˜¾ç¤ºè®­ç»ƒå®Œæˆä¿¡æ¯ï¼Œä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å®é™…çš„è¾“å‡ºç›®å½•
        try:
            with open(config_path_obj, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                actual_output_dir = config.get('output_dir', 'unknown')
        except:
            actual_output_dir = 'unknown'

        print("Training completed successfully!")
        print(f"Model saved to: {actual_output_dir}")
        print("Next steps:")
        print(f"{Style.BRIGHT}Test the trained model with 'dataflow chat'")

        return True

    except Exception as e:
        print(f"Training error: {e}")
        return False


def cli_pdf2model_chat(model_path=None, cache_path="./", base_model=None):
    """Start LlamaFactory chat interface"""
    print("Starting chat interface...")

    current_dir = Path(os.getcwd())

    # å¤„ç†cacheè·¯å¾„
    cache_path_obj = Path(cache_path)
    if not cache_path_obj.is_absolute():
        cache_path_obj = current_dir / cache_path_obj

    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if model_path is None:
        # è·å–æœ€æ–°çš„æ¨¡å‹ç›®å½•
        latest_model_dir = get_latest_model_dir(cache_path_obj)
        if latest_model_dir:
            model_path = latest_model_dir
        else:
            print("No trained model found")
            print("Run 'dataflow pdf2model train' to train a model first")
            return False

    model_path = Path(model_path)
    if not model_path.exists():
        print(f"Model not found: {model_path}")
        print("Run 'dataflow pdf2model train' to train a model first")
        return False

    # ç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„
    if base_model is None:
        # å°è¯•ä»è®­ç»ƒé…ç½®ä¸­è¯»å–åŸºç¡€æ¨¡å‹
        config_file = cache_path_obj / ".cache" / "train_config.yaml"
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    base_model = config.get('model_name_or_path', 'Qwen/Qwen2.5-7B-Instruct')
            except:
                base_model = 'Qwen/Qwen2.5-7B-Instruct'
        else:
            base_model = 'Qwen/Qwen2.5-7B-Instruct'

    # æ£€æŸ¥LlamaFactory
    try:
        import llamafactory
        print("LlamaFactory available")
    except ImportError:
        print("LlamaFactory not installed")
        print("Install with: pip install llamafactory[torch,metrics]")
        return False

    # ç›´æ¥ç”¨å‘½ä»¤è¡Œå‚æ•°å¯åŠ¨èŠå¤©
    chat_cmd = [
        "llamafactory-cli", "chat",
        "--model_name_or_path", base_model,
        "--adapter_name_or_path", str(model_path.absolute())
    ]

    print(f"Base model: {base_model}")
    print(f"Adapter path: {model_path}")
    print(f"Command: {' '.join(chat_cmd)}")
    print("-" * 60)
    print("Starting chat session...")
    print("-" * 60)

    try:
        result = subprocess.run(chat_cmd, check=True)
        print("\nâœ… Chat session completed")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ Chat failed: {e}")
        return False
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Chat session ended by user")
        return True